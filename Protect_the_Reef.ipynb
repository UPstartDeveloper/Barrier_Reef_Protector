{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Starter Code from Kaggle Notebook\nThis is just being stored here for historical reasons. Will remove later. It said this:\n\n```\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport sys\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, RUN THIS (by clicking run or pressing Shift+Enter) to list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n```","metadata":{"id":"BTIQPCjxcDVI"}},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"5wuAfvt8gvy8"}},{"cell_type":"code","source":"import os, sys\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf","metadata":{"id":"X8NJMKgqbfve","execution":{"iopub.status.busy":"2021-12-06T19:52:15.868149Z","iopub.execute_input":"2021-12-06T19:52:15.868812Z","iopub.status.idle":"2021-12-06T19:52:15.873833Z","shell.execute_reply.started":"2021-12-06T19:52:15.868769Z","shell.execute_reply":"2021-12-06T19:52:15.872994Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Double-check Versions","metadata":{"id":"ja73-dtbcPaJ"}},{"cell_type":"code","source":"print(sys.version)  # Python version\nprint(tf.__version__) # on Kaggle, this will be 2.6","metadata":{"id":"dMvucPqKcCiC","outputId":"10c2d0de-1110-425f-b190-05942ed89964","execution":{"iopub.status.busy":"2021-12-06T19:52:17.959791Z","iopub.execute_input":"2021-12-06T19:52:17.960124Z","iopub.status.idle":"2021-12-06T19:52:17.965185Z","shell.execute_reply.started":"2021-12-06T19:52:17.960090Z","shell.execute_reply":"2021-12-06T19:52:17.964511Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis\n\n*Abbreviations*:     \n- COTS = \"Crown-of-thorns starfish\"\n\nNote: to locate where the dataset is located (on this Kaggle kernel) - I clicked on the \"copy\" button icon next to the dataset folder icon in the \"Data\" tab.","metadata":{"id":"xgRbIFh0g3h0"}},{"cell_type":"code","source":"data_path = '../input/tensorflow-great-barrier-reef'\ndf = pd.read_csv(f\"{data_path}/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:53:34.800422Z","iopub.execute_input":"2021-12-06T19:53:34.801217Z","iopub.status.idle":"2021-12-06T19:53:34.836150Z","shell.execute_reply.started":"2021-12-06T19:53:34.801166Z","shell.execute_reply":"2021-12-06T19:53:34.835554Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T20:07:09.850280Z","iopub.execute_input":"2021-12-06T20:07:09.851142Z","iopub.status.idle":"2021-12-06T20:07:09.863906Z","shell.execute_reply.started":"2021-12-06T20:07:09.851090Z","shell.execute_reply":"2021-12-06T20:07:09.862929Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### How to Be an Observant Surveyor\n\nMy goal in this analysis is to build an object-detection system that can scale up the efforts of manual surveyors in the Great Barrier Reef. With that in mind... what makes a human surveyor great at spotting COTS in the first place?","metadata":{"id":"AkMbPlRLg7nT"}},{"cell_type":"markdown","source":"**Question 1**: Do the COTS tend to lump close together?\n\n*Part 1:* On average, how many COTS are seen together in a single video frame?\nTo do this, let's start by first adding a column with the counts of COTS seen in each particular frame:","metadata":{"id":"FW8-Q0t6hgxt"}},{"cell_type":"code","source":"type(df['annotations'][43])  # although the data type visually looks like a list, the CSV is all text","metadata":{"execution":{"iopub.status.busy":"2021-12-06T20:07:18.234580Z","iopub.execute_input":"2021-12-06T20:07:18.235058Z","iopub.status.idle":"2021-12-06T20:07:18.240237Z","shell.execute_reply.started":"2021-12-06T20:07:18.235006Z","shell.execute_reply":"2021-12-06T20:07:18.239523Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"We know there may be multiple COTS spotted in a single frame, so let's count up each that is spotted in a new column. We'll using the `{` to know how many COTS are in each: ","metadata":{}},{"cell_type":"code","source":"count_func = lambda string: string.count('{')\nspotted = df['annotations'].apply(count_func)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T20:12:27.614899Z","iopub.execute_input":"2021-12-06T20:12:27.615216Z","iopub.status.idle":"2021-12-06T20:12:27.634396Z","shell.execute_reply.started":"2021-12-06T20:12:27.615180Z","shell.execute_reply":"2021-12-06T20:12:27.633628Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"df = df.assign(starfish_spotted=spotted)\ndf.head()","metadata":{"id":"NABW28cMhzMk","execution":{"iopub.status.busy":"2021-12-06T20:13:52.485148Z","iopub.execute_input":"2021-12-06T20:13:52.485482Z","iopub.status.idle":"2021-12-06T20:13:52.498523Z","shell.execute_reply.started":"2021-12-06T20:13:52.485443Z","shell.execute_reply":"2021-12-06T20:13:52.497567Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"Cool beans! Now we can calculate the average of COTS spotted in a given frame:","metadata":{}},{"cell_type":"code","source":"mean_starfish_spotted_in_a_frame = round(df[\"starfish_spotted\"].mean(), 4)\nprint(f\"On average, {mean_starfish_spotted_in_a_frame} COTS are seen together in a single video frame.\")","metadata":{"execution":{"iopub.status.busy":"2021-12-06T20:16:35.498469Z","iopub.execute_input":"2021-12-06T20:16:35.499274Z","iopub.status.idle":"2021-12-06T20:16:35.503861Z","shell.execute_reply.started":"2021-12-06T20:16:35.499239Z","shell.execute_reply":"2021-12-06T20:16:35.503273Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"Wowza, that seems very low. Let's also visualize the distribution of the `starfish_spotted` column using a histogram and PDF:","metadata":{}},{"cell_type":"code","source":"def plot_histogram(df, column, title, x_axis, y_axis):\n    \"\"\"\n    Plots the PDF of a column in a given DataFrame, using Matplotlib.\n    \n    Credit for the equation used for plotting the PDF goes to the NumPy documentation:\n        https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html\n    \n    Args:\n        df(pandas.DataFrame)\n        column(str): name of the column being plotted\n        title(str), x_axis(str), y_axis(str): will be added to the plot\n        \n    Returns: None\n    \"\"\"\n    # A: calculate the mean and std dev of the column\n    mu, sigma = df[column].mean(), df[column].std()\n    # B: init the histogram\n    bin_edges, bins_probabilites, ignored = plt.hist(df[column], density=True)\n    # C: plot the PDF \n    plt.plot(bins_probabilites, 1/(sigma * np.sqrt(2 * np.pi)) *\n               np.exp(-(bins_probabilites - mu)**2 / (2 * sigma**2)),\n             linewidth=2, color='r')\n    # D: make the plot more presentable\n    plt.title(title)\n    plt.xlabel(x_axis)\n    plt.ylabel(y_axis)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T20:36:52.449828Z","iopub.execute_input":"2021-12-06T20:36:52.450114Z","iopub.status.idle":"2021-12-06T20:36:52.456899Z","shell.execute_reply.started":"2021-12-06T20:36:52.450084Z","shell.execute_reply":"2021-12-06T20:36:52.456292Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"plot_histogram(df, 'starfish_spotted', \"PDF of COTS Spotted in Video Frames\", \"No. of COTS\", \"Probability\")","metadata":{"execution":{"iopub.status.busy":"2021-12-06T20:29:57.972770Z","iopub.execute_input":"2021-12-06T20:29:57.973058Z","iopub.status.idle":"2021-12-06T20:29:58.136190Z","shell.execute_reply.started":"2021-12-06T20:29:57.973025Z","shell.execute_reply":"2021-12-06T20:29:58.135568Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"One takeaway on this: the distribution of COTS per video frame is heavily skewed, and the majority of them have none at all. This reinforces the idea that we'll want to weigh the `recall` highly in evaluating the eventual model we build, so we can detect the relatively low number of COTS that exist per image.","metadata":{}},{"cell_type":"markdown","source":"*Part 2:* On average, how many video frames do we go without seeing any COTS in the provided videos?","metadata":{"id":"nO2Vr_fAhzk7"}},{"cell_type":"code","source":"pass","metadata":{"id":"imZE_hTuh-ji"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Question 2:** What are some \"giveaways\" that a certain object in an video frame is that of a COTS?\n\n- *Part 1:* What is the distibution of the observed colors of COTS in the videos?","metadata":{"id":"a29qvuhOh-5A"}},{"cell_type":"code","source":"pass","metadata":{"id":"_0yCuOIyi1L_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- *Part 2:* What is the distibution of the observed sizes of COTS in the videos?","metadata":{"id":"pc4Nmd6ei1pw"}},{"cell_type":"code","source":"pass","metadata":{"id":"7oy5aiL1i4PH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling\n\nStep 1: Colab-specific: Mount Google Drive (we can do this by clicking the appropiate icons on the screen, please see here for more info)","metadata":{"id":"9D4YhwbDceiK"}},{"cell_type":"code","source":"try:\n  from google.colab import drive\n  drive.mount('/content/drive')\nexcept ModuleNotFoundError:\n  pass","metadata":{"id":"8ie-ZPMMfmK6","outputId":"87219983-b5b8-430e-9e7a-6cb9023e3be6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Colab-specific - find a folder to store YOLOv5 stuff\n%cd ./drive/MyDrive","metadata":{"id":"Rn4HYy82cQEq","outputId":"a8b4336c-f971-456f-da42-ec7527581897"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step 2:** Now let's load the YOLOv5 model from the PyTorch Hub:","metadata":{"id":"U7Jxmx77pST0"}},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5  # clone\n%cd yolov5\n%pip install -qr requirements.txt  # install","metadata":{"id":"_ElWlt5Vofxl","outputId":"ce560c94-6b08-4a41-ccbd-08657a34cfbf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"id":"EYVbFjR8f9Q5","outputId":"f113fb3e-e21f-4da4-8966-08279040c827"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import utils\ndisplay = utils.notebook_init()  # running the checks","metadata":{"id":"Ux8otJbFsE4d","outputId":"0c05a743-77a8-4fab-d02f-9223a3c4827a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step 3:** Now, let export to a `SavedModel` (this is using the \"nano\" version of YOLOv5, the smallest). Note that only need to do this *once*:","metadata":{"id":"OGbp452cpp1C"}},{"cell_type":"code","source":"!python export.py --weights yolov5n.pt --include saved_model","metadata":{"id":"FM_p2nh2ptm2","outputId":"c7103709-d637-4086-f0e6-28f4f329ab42"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step 4:** Now, let's test loading the model back in via Tensorflow 2:","metadata":{"id":"h1TK9CS0tWZZ"}},{"cell_type":"code","source":"tf_model = tf.keras.models.load_model('./yolov5n_saved_model')","metadata":{"id":"5CNlkCvPsfrp","outputId":"37e6a138-5052-404f-98f2-8d24ac586db7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Woohoo! We got a warning that the model has no training configuration, but that is to be expected (since we want to do all the training code henceforth ourselves, using Tensorflow code). \n\nLet's get started!","metadata":{"id":"gm9_kzUBuINz"}},{"cell_type":"code","source":"","metadata":{"id":"3zARnYg9keEb"},"execution_count":null,"outputs":[]}]}